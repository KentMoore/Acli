from concurrent.futures import ThreadPoolExecutor
import asyncio
import csv
import math
import threading
import time

from DrissionPage import ChromiumPage
from DrissionPage.errors import ContextLostError

# ========================
# é…ç½®
# ========================
MAX_PAGE = 332   # æ€»é¡µæ•°
BASE_URL = 'https://www.lanqiao.cn/problems/?first_category_id=1&sort=students_count&page={}'
MAX_WORKERS = 4  # åŒæ—¶å¼€çš„â€œæ ‡ç­¾é¡µçº¿ç¨‹â€æ•°é‡ï¼Œå»ºè®® 2~6 ä¹‹é—´è¯•
OUTPUT_CSV = 'lanqiao_problems_all_drission_async.csv'


def split_pages(max_page: int, n_workers: int):
    """æŠŠ 1..max_page å‡åŒ€åˆ†æˆ n_workers ä»½"""
    pages = list(range(1, max_page + 1))
    n_workers = min(n_workers, len(pages))
    size = math.ceil(len(pages) / n_workers)
    return [pages[i:i + size] for i in range(0, len(pages), size)]


def crawl_pages(tab, pages_subset):
    """
    åœ¨ä¸€ä¸ªçº¿ç¨‹é‡Œä½¿ç”¨ä¸€ä¸ªæ ‡ç­¾é¡µ(tab)é‡‡é›†è‹¥å¹²é¡µã€‚
    tab: ChromiumTab æˆ– ChromiumPage å¯¹è±¡
    """
    thread_name = threading.current_thread().name
    print(f"[{thread_name}] å¯åŠ¨ï¼Œè´Ÿè´£é¡µç ï¼š{pages_subset[0]} ~ {pages_subset[-1]}")

    all_rows = []

    for p in pages_subset:
        url = BASE_URL.format(p)

        # æ¯ä¸€é¡µæœ€å¤šé‡è¯• 3 æ¬¡ï¼Œé˜²æ­¢ ContextLostError ç›´æ¥æŠŠç¨‹åºå¹²å´©
        for attempt in range(1, 4):
            try:
                print(f"[{thread_name}] æŠ“å–ç¬¬ {p} é¡µ (ç¬¬ {attempt} æ¬¡å°è¯•)ï¼š{url}")
                tab.get(url)
                tab.wait.doc_loaded()   # ç­‰é¡µé¢åŠ è½½å®Œæˆ
                time.sleep(0.8)         # ç¨å¾®æ­‡ä¸€ä¸‹ï¼Œé¿å…å¤ªçŒ›è§¦å‘é£æ§

                items = tab.eles('css:div.problem-item-wrapper')
                if not items:
                    print(f"[{thread_name}] âš ï¸ ç¬¬ {p} é¡µæ²¡æœ‰æŠ“åˆ°ä»»ä½•é¢˜ç›®ï¼ˆå¯èƒ½æ˜¯æœªç™»å½•/é£æ§/é¡µæ•°è¶…å‡ºï¼‰")
                    break

                print(f"[{thread_name}] âœ… ç¬¬ {p} é¡µå…± {len(items)} é“é¢˜")

                page_rows = []

                for item in items:
                    # é¢˜å·
                    id_ele = item.ele('css:span.problem-id', timeout=0)
                    pid = id_ele.text.strip() if id_ele else ''

                    # é¢˜ç›®åç§°
                    title_ele = item.ele('css:span.name.is-open', timeout=0)
                    if title_ele:
                        title = (title_ele.attr('title') or title_ele.text or '').strip()
                    else:
                        title = ''

                    # éš¾åº¦
                    level_ele = item.ele('css:span.level-text', timeout=0)
                    level = level_ele.text.strip() if level_ele else 'æœªçŸ¥'

                    # é€šè¿‡ç‡
                    percent_ele = item.ele('css:span.meta-percent', timeout=0)
                    percent = percent_ele.text.strip() if percent_ele else 'æœªçŸ¥'

                    # æŒ‘æˆ˜äººæ•°
                    user_ele = item.ele('css:span.meta-users', timeout=0)
                    users = user_ele.text.strip() if user_ele else 'æœªçŸ¥'

                    # æ ‡ç­¾
                    tag_eles = item.eles('css:div.problem-tags span.tag')
                    tags = [t.text.strip() for t in tag_eles]
                    tag_str = 'ã€'.join(tags) if tags else ''

                    page_rows.append((p, pid, title, level, percent, users, tag_str))

                all_rows.extend(page_rows)
                # å½“å‰é¡µæˆåŠŸäº†å°±è·³å‡ºé‡è¯•å¾ªç¯
                break

            except ContextLostError:
                # å®˜æ–¹è¯´æ˜ï¼šContextLostError æ˜¯â€œé¡µé¢è¢«åˆ·æ–°â€ï¼Œéœ€è¦ç­‰å¾…åé‡è¯• :contentReference[oaicite:2]{index=2}
                print(f"[{thread_name}] âš ï¸ ç¬¬ {p} é¡µå‡ºç° ContextLostErrorï¼Œå¯èƒ½æ˜¯é¡µé¢åˆ·æ–°äº†ï¼Œå‡†å¤‡é‡è¯•...")
                time.sleep(1.5)

                if attempt == 3:
                    print(f"[{thread_name}] âŒ ç¬¬ {p} é¡µé‡è¯• 3 æ¬¡ä»å¤±è´¥ï¼Œè·³è¿‡è¯¥é¡µã€‚")

            except Exception as e:
                print(f"[{thread_name}] âŒ ç¬¬ {p} é¡µå‡ºç°å…¶ä»–å¼‚å¸¸ï¼š{e}")
                if attempt == 3:
                    print(f"[{thread_name}] âŒ ç¬¬ {p} é¡µé‡è¯• 3 æ¬¡ä»å¤±è´¥ï¼Œè·³è¿‡è¯¥é¡µã€‚")
                else:
                    time.sleep(1.5)

    print(f"[{thread_name}] ç»“æŸï¼ŒæŠ“åˆ° {len(all_rows)} æ¡è®°å½•")
    return all_rows


def create_tabs(page: ChromiumPage, n_workers: int):
    """
    æŒ‰å®˜ç½‘â€œå¤šçº¿ç¨‹æ“ä½œæ ‡ç­¾é¡µâ€çš„å†™æ³•ï¼Œåœ¨ä¸€ä¸ªæµè§ˆå™¨é‡Œå¼€å¤šä¸ªæ ‡ç­¾é¡µï¼Œæ¯ä¸ªçº¿ç¨‹ç”¨ä¸€ä¸ªã€‚:contentReference[oaicite:3]{index=3}
    """
    tabs = []

    # ç¬¬ä¸€ä¸ªæ ‡ç­¾é¡µï¼šå½“å‰è¿™ä¸ª page æœ¬èº«å°±å¯ä»¥å½“ä½œä¸€ä¸ª tab ä½¿ç”¨
    tab0 = page.get_tab()
    tabs.append(tab0)

    # å†æ–°å»ºæ ‡ç­¾é¡µ
    for _ in range(n_workers - 1):
        # new_tab è¿”å›çš„æ˜¯ tab çš„ idï¼Œè¿™é‡Œå…ˆå»ºç©ºç™½é¡µï¼Œåç»­çº¿ç¨‹é‡Œå† get å®é™… URL
        tab_id = page.new_tab('about:blank')
        tab = page.get_tab(tab_id)
        tabs.append(tab)

    return tabs


async def async_crawl_all(page: ChromiumPage):
    page_splits = split_pages(MAX_PAGE, MAX_WORKERS)
    num_workers = len(page_splits)
    print(f"æ€»é¡µæ•° {MAX_PAGE}ï¼Œæ‹†æˆ {num_workers} ä¸ªä»»åŠ¡ã€‚")

    # æŒ‰ä»»åŠ¡æ•°åˆ›å»ºå¯¹åº”æ•°é‡çš„æ ‡ç­¾é¡µ
    tabs = create_tabs(page, num_workers)

    loop = asyncio.get_running_loop()
    with ThreadPoolExecutor(max_workers=num_workers) as executor:
        tasks = [
            loop.run_in_executor(executor, crawl_pages, tabs[i], subset)
            for i, subset in enumerate(page_splits)
        ]
        all_batches = await asyncio.gather(*tasks)

    # ç»Ÿä¸€å…³æ‰å¤šå¼€çš„æ ‡ç­¾é¡µï¼ˆç¬¬ä¸€ä¸ª tab å°±æ˜¯ pageï¼Œå¯ä»¥ç•™ç€ä¹Ÿæ— æ‰€è°“ï¼‰
    for t in tabs[1:]:
        try:
            t.close()
        except Exception:
            pass

    return all_batches


def save_to_csv(all_batches):
    # å±•å¼€äºŒç»´åˆ—è¡¨
    rows = [row for batch in all_batches for row in batch]

    # æŒ‰ (é¡µç , é¢˜å·) æ’åºï¼Œä¿è¯é¡ºåºç¨³å®š
    rows.sort(key=lambda x: (x[0], int(x[1]) if str(x[1]).isdigit() else 999999))

    print("-" * 80)
    print(f"æ±‡æ€»åå…± {len(rows)} é“é¢˜ç›®ï¼Œå¼€å§‹å†™å…¥ CSVï¼š{OUTPUT_CSV}")

    with open(OUTPUT_CSV, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.writer(f)
        writer.writerow(['åºå·', 'é¢˜å·', 'é¢˜ç›®åç§°', 'éš¾åº¦', 'é€šè¿‡ç‡', 'æŒ‘æˆ˜äººæ•°', 'æ ‡ç­¾', 'æ‰€åœ¨é¡µç '])

        for idx, (page_no, pid, title, level, percent, users, tag_str) in enumerate(rows, start=1):
            writer.writerow([idx, pid, title, level, percent, users, tag_str, page_no])

    print("ğŸ’¾ å†™å…¥å®Œæˆï¼")


if __name__ == '__main__':
    t0 = time.time()
    print("å¼€å§‹å¤šçº¿ç¨‹ + asyncio æŠ“å–è“æ¡¥é¢˜ç›®åˆ—è¡¨...")

    # âœ… åªåˆ›å»ºä¸€ä¸ªé¡µé¢å¯¹è±¡ï¼Œåé¢æ‰€æœ‰çº¿ç¨‹éƒ½é€šè¿‡ä¸åŒ tab æ¥ç”¨å®ƒ
    page = ChromiumPage()

    try:
        batches = asyncio.run(async_crawl_all(page))
        save_to_csv(batches)
    finally:
        # âœ… åªåœ¨ä¸»çº¿ç¨‹ç»Ÿä¸€ quit ä¸€æ¬¡
        try:
            page.quit()
        except Exception:
            pass

    t1 = time.time()
    print(f"å…¨éƒ¨å®Œæˆï¼Œç”¨æ—¶ {t1 - t0:.2f} ç§’")
