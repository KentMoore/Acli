from concurrent.futures import ThreadPoolExecutor
import asyncio
import csv
import math
import threading
import time

from DrissionPage import ChromiumPage
from DrissionPage.errors import ContextLostError

# ========================
# é…ç½®
# ========================
MAX_PAGE = 332   # åˆ—è¡¨æ€»é¡µæ•°
BASE_URL = 'https://www.lanqiao.cn/problems/?first_category_id=1&sort=students_count&page={}'
DETAIL_URL = 'https://www.lanqiao.cn/problems/{pid}/learning/?page=1&first_category_id=1'

MAX_WORKERS = 4        # åˆ—è¡¨é¡µ çº¿ç¨‹ / æ ‡ç­¾é¡µ æ•°é‡
DETAIL_WORKERS = 4     # è¯¦æƒ…é¡µ çº¿ç¨‹ / æ ‡ç­¾é¡µ æ•°é‡
OUTPUT_CSV = 'lanqiao_problems_fulltext.csv'   # è¾“å‡ºæ–‡ä»¶å


# ========================
# å·¥å…·å‡½æ•°
# ========================
def split_pages(max_page: int, n_workers: int):
    """æŠŠ 1..max_page å‡åŒ€åˆ†æˆ n_workers ä»½"""
    pages = list(range(1, max_page + 1))
    n_workers = min(n_workers, len(pages))
    size = math.ceil(len(pages) / n_workers)
    return [pages[i:i + size] for i in range(0, len(pages), size)]


def split_list(items, n_workers: int):
    """é€šç”¨åˆ—è¡¨åˆ‡åˆ†ï¼Œç”¨äºæŒ‰é¢˜ç›®åˆ‡åˆ†è¯¦æƒ…ä»»åŠ¡"""
    if not items:
        return []
    n_workers = min(n_workers, len(items))
    size = math.ceil(len(items) / n_workers)
    return [items[i:i + size] for i in range(0, len(items), size)]


def get_page_full_text(tab):
    """
    å°½é‡è·å–ã€é¢˜ç›®ä¸»è¦åŒºåŸŸã€‘çš„æ‰€æœ‰æ–‡æœ¬ï¼›
    å¦‚æœå®šä½ä¸åˆ°ï¼Œå°±é€€è€Œæ±‚å…¶æ¬¡æ‹¿æ•´ä¸ª body çš„æ–‡æœ¬ï¼Œå†ä¸è¡Œå°± html åŸæ–‡ã€‚
    """
    # å…ˆè¯•ä¸€äº›å¯èƒ½çš„â€œä¸»ä½“å®¹å™¨â€
    candidate_selectors = [
        'div.problem-main',
        'div.problem-detail',
        'div.problem',          # è“æ¡¥ç»å¸¸ç”¨ problem è¿™ä¸ªç±»
        'main',
        'div.main',
        'div.layout-main',
    ]

    for sel in candidate_selectors:
        try:
            ele = tab.ele(f'css:{sel}', timeout=0.8)
            if ele:
                txt = (ele.text or '').strip()
                # å¤ªçŸ­è¯´æ˜ä¸æ˜¯æ­£æ–‡ï¼Œç»§ç»­æ‰¾
                if txt and len(txt) > 50:
                    return txt
        except Exception:
            pass

    # é€€è€Œæ±‚å…¶æ¬¡ï¼šæ•´ä¸ª body æ–‡æœ¬
    try:
        body = tab.ele('tag:body', timeout=1.5)
        if body:
            txt = (body.text or '').strip()
            if txt:
                return txt
    except Exception:
        pass

    # æœ€åå…œåº•ï¼šæ•´é¡µ HTMLï¼ˆå¯èƒ½æœ‰æ ‡ç­¾ï¼Œè¦ä½ åå¤„ç†ï¼‰
    try:
        html = (tab.html or '').strip()
        return html
    except Exception:
        return ''


# ========================
# ç¬¬ä¸€é˜¶æ®µï¼šæŠ“å–é¢˜ç›®åˆ—è¡¨
# ========================
def crawl_pages(tab, pages_subset):
    """
    åœ¨ä¸€ä¸ªçº¿ç¨‹é‡Œä½¿ç”¨ä¸€ä¸ªæ ‡ç­¾é¡µ(tab)é‡‡é›†è‹¥å¹²é¡µåˆ—è¡¨ã€‚
    """
    thread_name = threading.current_thread().name
    print(f"[{thread_name}] åˆ—è¡¨çº¿ç¨‹å¯åŠ¨ï¼Œè´Ÿè´£é¡µç ï¼š{pages_subset[0]} ~ {pages_subset[-1]}")

    all_rows = []

    for p in pages_subset:
        url = BASE_URL.format(p)

        # æ¯ä¸€é¡µæœ€å¤šé‡è¯• 3 æ¬¡
        for attempt in range(1, 4):
            try:
                print(f"[{thread_name}] æŠ“å–ç¬¬ {p} é¡µ (ç¬¬ {attempt} æ¬¡å°è¯•)ï¼š{url}")
                tab.get(url)
                tab.wait.doc_loaded()
                time.sleep(0.8)

                items = tab.eles('css:div.problem-item-wrapper')
                if not items:
                    print(f"[{thread_name}] âš ï¸ ç¬¬ {p} é¡µæ²¡æœ‰æŠ“åˆ°ä»»ä½•é¢˜ç›®ï¼ˆå¯èƒ½æœªç™»å½•/é£æ§/é¡µæ•°è¶…å‡ºï¼‰")
                    break

                print(f"[{thread_name}] âœ… ç¬¬ {p} é¡µå…± {len(items)} é“é¢˜")

                page_rows = []

                for item in items:
                    # é¢˜å·
                    id_ele = item.ele('css:span.problem-id', timeout=0)
                    pid = id_ele.text.strip() if id_ele else ''

                    # é¢˜ç›®åç§°
                    title_ele = item.ele('css:span.name.is-open', timeout=0)
                    if title_ele:
                        title = (title_ele.attr('title') or title_ele.text or '').strip()
                    else:
                        title = ''

                    # éš¾åº¦
                    level_ele = item.ele('css:span.level-text', timeout=0)
                    level = level_ele.text.strip() if level_ele else 'æœªçŸ¥'

                    # é€šè¿‡ç‡
                    percent_ele = item.ele('css:span.meta-percent', timeout=0)
                    percent = percent_ele.text.strip() if percent_ele else 'æœªçŸ¥'

                    # æŒ‘æˆ˜äººæ•°
                    user_ele = item.ele('css:span.meta-users', timeout=0)
                    users = user_ele.text.strip() if user_ele else 'æœªçŸ¥'

                    # æ ‡ç­¾
                    tag_eles = item.eles('css:div.problem-tags span.tag')
                    tags = [t.text.strip() for t in tag_eles]
                    tag_str = 'ã€'.join(tags) if tags else ''

                    page_rows.append((p, pid, title, level, percent, users, tag_str))

                all_rows.extend(page_rows)
                break  # å½“å‰é¡µå·²æˆåŠŸï¼Œæ— éœ€é‡è¯•

            except ContextLostError:
                print(f"[{thread_name}] âš ï¸ ç¬¬ {p} é¡µ ContextLostErrorï¼Œå‡†å¤‡é‡è¯•...")
                time.sleep(1.5)
                if attempt == 3:
                    print(f"[{thread_name}] âŒ ç¬¬ {p} é¡µé‡è¯• 3 æ¬¡ä»å¤±è´¥ï¼Œè·³è¿‡ã€‚")

            except Exception as e:
                print(f"[{thread_name}] âŒ ç¬¬ {p} é¡µå¼‚å¸¸ï¼š{e}")
                if attempt == 3:
                    print(f"[{thread_name}] âŒ ç¬¬ {p} é¡µé‡è¯• 3 æ¬¡ä»å¤±è´¥ï¼Œè·³è¿‡ã€‚")
                else:
                    time.sleep(1.5)

    print(f"[{thread_name}] åˆ—è¡¨çº¿ç¨‹ç»“æŸï¼Œå…±æŠ“ {len(all_rows)} æ¡ã€‚")
    return all_rows


def create_tabs(page: ChromiumPage, n_workers: int):
    """
    åœ¨ä¸€ä¸ªæµè§ˆå™¨é‡Œå¼€å¤šä¸ªæ ‡ç­¾é¡µï¼Œæ¯ä¸ªçº¿ç¨‹ç”¨ä¸€ä¸ªã€‚
    """
    tabs = []

    # å½“å‰è¿™ä¸ª page çš„ tab ä¹Ÿæ‹¿æ¥ç”¨
    tab0 = page.get_tab()
    tabs.append(tab0)

    # å†æ–°å»ºæ ‡ç­¾é¡µ
    for _ in range(n_workers - 1):
        tab_id = page.new_tab('about:blank')
        tab = page.get_tab(tab_id)
        tabs.append(tab)

    return tabs


async def async_crawl_all(page: ChromiumPage):
    """å¼‚æ­¥è°ƒåº¦ + çº¿ç¨‹æ± ï¼ŒæŠ“å®Œæ‰€æœ‰åˆ—è¡¨é¡µ"""
    page_splits = split_pages(MAX_PAGE, MAX_WORKERS)
    num_workers = len(page_splits)
    print(f"æ€»é¡µæ•° {MAX_PAGE}ï¼Œæ‹†æˆ {num_workers} ä¸ªåˆ—è¡¨ä»»åŠ¡ã€‚")

    tabs = create_tabs(page, num_workers)

    loop = asyncio.get_running_loop()
    with ThreadPoolExecutor(max_workers=num_workers) as executor:
        tasks = [
            loop.run_in_executor(executor, crawl_pages, tabs[i], subset)
            for i, subset in enumerate(page_splits)
        ]
        all_batches = await asyncio.gather(*tasks)

    # å…³æ‰å¤šå¼€çš„ tabï¼ˆç¬¬ä¸€ä¸ªæ˜¯ page æœ¬èº«ï¼Œå¯ä»¥ä¿ç•™ï¼‰
    for t in tabs[1:]:
        try:
            t.close()
        except Exception:
            pass

    return all_batches


def flatten_batches_to_problems(all_batches):
    """
    å±•å¼€æ‰¹æ¬¡ï¼Œå˜æˆæ–¹ä¾¿åç»­å¤„ç†çš„é¢˜ç›®åˆ—è¡¨ï¼ˆdict åˆ—è¡¨ï¼‰
    """
    rows = [row for batch in all_batches for row in batch]
    # æŒ‰ (é¡µç , é¢˜å·) æ’ä¸ªåº
    rows.sort(key=lambda x: (x[0], int(x[1]) if str(x[1]).isdigit() else 999999))

    problems = []
    for idx, (page_no, pid, title, level, percent, users, tag_str) in enumerate(rows, start=1):
        problems.append(
            {
                'idx': idx,
                'page_no': page_no,
                'pid': pid,
                'title': title,
                'level': level,
                'percent': percent,
                'users': users,
                'tags': tag_str,
                'detail_url': DETAIL_URL.format(pid=pid) if pid else '',
                'full_text': '',   # ç¬¬äºŒé˜¶æ®µå¡«
            }
        )
    return problems


# ========================
# ç¬¬äºŒé˜¶æ®µï¼šæŠ“å–é¢˜ç›®è¯¦æƒ…å…¨æ–‡
# ========================
def crawl_detail_batch(tab, problems_subset):
    """
    åœ¨ä¸€ä¸ªçº¿ç¨‹é‡Œç”¨ä¸€ä¸ªæ ‡ç­¾é¡µæŠ“è‹¥å¹²é¢˜ç›®çš„è¯¦æƒ…å…¨æ–‡ã€‚
    """
    thread_name = threading.current_thread().name
    print(f"[{thread_name}] è¯¦æƒ…çº¿ç¨‹å¯åŠ¨ï¼Œè¦æŠ“ {len(problems_subset)} é“é¢˜ã€‚")

    for problem in problems_subset:
        pid = problem['pid']
        if not pid:
            continue

        url = problem['detail_url'] or DETAIL_URL.format(pid=pid)
        problem['detail_url'] = url

        for attempt in range(1, 4):
            try:
                print(f"[{thread_name}] æŠ“å–é¢˜å· {pid} (ç¬¬ {attempt} æ¬¡)ï¼š{url}")
                tab.get(url)
                tab.wait.doc_loaded()
                time.sleep(0.8)

                full_text = get_page_full_text(tab)
                problem['full_text'] = full_text

                break   # æˆåŠŸå°±é€€å‡ºé‡è¯•å¾ªç¯

            except ContextLostError:
                print(f"[{thread_name}] âš ï¸ é¢˜å· {pid} ContextLostErrorï¼Œå‡†å¤‡é‡è¯•...")
                time.sleep(1.5)
                if attempt == 3:
                    print(f"[{thread_name}] âŒ é¢˜å· {pid} é‡è¯• 3 æ¬¡ä»å¤±è´¥ï¼Œè·³è¿‡ã€‚")

            except Exception as e:
                print(f"[{thread_name}] âŒ æŠ“é¢˜å· {pid} å¼‚å¸¸ï¼š{e}")
                if attempt == 3:
                    print(f"[{thread_name}] âŒ é¢˜å· {pid} é‡è¯• 3 æ¬¡ä»å¤±è´¥ï¼Œè·³è¿‡ã€‚")
                else:
                    time.sleep(1.5)

    print(f"[{thread_name}] è¯¦æƒ…çº¿ç¨‹ç»“æŸã€‚")
    return problems_subset


def crawl_all_details(page: ChromiumPage, problems, max_workers: int = DETAIL_WORKERS):
    """
    å¤šçº¿ç¨‹ + å¤šæ ‡ç­¾é¡µï¼ŒæŠ“æ‰€æœ‰é¢˜ç›®çš„è¯¦æƒ…å…¨æ–‡ã€‚
    """
    if not problems:
        return problems

    splits = split_list(problems, max_workers)
    num_workers = len(splits)
    print(f"å…± {len(problems)} é“é¢˜ï¼Œæ‹†æˆ {num_workers} ä¸ªè¯¦æƒ…ä»»åŠ¡ã€‚")

    tabs = create_tabs(page, num_workers)

    results = []
    with ThreadPoolExecutor(max_workers=num_workers) as executor:
        futures = [
            executor.submit(crawl_detail_batch, tabs[i], subset)
            for i, subset in enumerate(splits)
        ]
        for fut in futures:
            results.append(fut.result())

    # å…³æ‰å¤šå¼€çš„ tab
    for t in tabs[1:]:
        try:
            t.close()
        except Exception:
            pass

    merged = [item for batch in results for item in batch]
    merged.sort(key=lambda x: x['idx'])
    return merged


# ========================
# ä¿å­˜åˆ° CSV
# ========================
def save_to_csv(problems):
    print("-" * 80)
    print(f"æ±‡æ€»åå…± {len(problems)} é“é¢˜ç›®ï¼ˆå«å…¨æ–‡ï¼‰ï¼Œå†™å…¥ CSVï¼š{OUTPUT_CSV}")

    with open(OUTPUT_CSV, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.writer(f)

        # æ³¨æ„ï¼šfull_text è¿™ä¸€åˆ—ä¼šæœ‰å¾ˆå¤šæ¢è¡Œï¼ŒCSV ä¼šè‡ªåŠ¨åŠ å¼•å·åŒ…èµ·æ¥
        writer.writerow(
            [
                'åºå·',
                'é¢˜å·',
                'é¢˜ç›®åç§°',
                'éš¾åº¦',
                'é€šè¿‡ç‡',
                'æŒ‘æˆ˜äººæ•°',
                'æ ‡ç­¾',
                'æ‰€åœ¨é¡µç ',
                'é¢˜ç›®é“¾æ¥',
                'full_text',  # é¢˜ç›®è¯¦æƒ…é¡µçš„æ‰€æœ‰æ–‡æœ¬
            ]
        )

        for item in problems:
            writer.writerow(
                [
                    item['idx'],
                    item['pid'],
                    item['title'],
                    item['level'],
                    item['percent'],
                    item['users'],
                    item['tags'],
                    item['page_no'],
                    item['detail_url'],
                    item.get('full_text', ''),
                ]
            )

    print("ğŸ’¾ å†™å…¥å®Œæˆï¼")


# ========================
# ä¸»å…¥å£
# ========================
if __name__ == '__main__':
    t0 = time.time()
    print("å¼€å§‹å¤šçº¿ç¨‹ + asyncio æŠ“å–è“æ¡¥é¢˜ç›®ã€åˆ—è¡¨ + è¯¦æƒ…å…¨æ–‡ã€‘...")

    page = ChromiumPage()   # ä½¿ç”¨ä½ æœ¬æœºçš„æµè§ˆå™¨é…ç½®ï¼Œç¡®ä¿å·²ç™»å½•è“æ¡¥

    try:
        # ç¬¬ä¸€é˜¶æ®µï¼šåˆ—è¡¨é¡µ
        list_batches = asyncio.run(async_crawl_all(page))
        problems = flatten_batches_to_problems(list_batches)

        # ç¬¬äºŒé˜¶æ®µï¼šé¢˜ç›®è¯¦æƒ…å…¨æ–‡
        problems = crawl_all_details(page, problems)

        # ä¿å­˜åˆ° CSV
        save_to_csv(problems)

    finally:
        try:
            page.quit()
        except Exception:
            pass

    t1 = time.time()
    print(f"å…¨éƒ¨å®Œæˆï¼Œç”¨æ—¶ {t1 - t0:.2f} ç§’")
