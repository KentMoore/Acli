from concurrent.futures import ThreadPoolExecutor
import asyncio
import csv
import math
import threading
import time

from DrissionPage import ChromiumPage
from DrissionPage.errors import ContextLostError

# ========================
# é…ç½®
# ========================
MAX_PAGE = 332   # åˆ—è¡¨æ€»é¡µæ•°
BASE_URL = 'https://www.lanqiao.cn/problems/?first_category_id=1&sort=students_count&page={}'
DETAIL_URL = 'https://www.lanqiao.cn/problems/{pid}/learning/?page=1&first_category_id=1'

MAX_WORKERS = 4          # åˆ—è¡¨é¡µâ€œæ ‡ç­¾é¡µçº¿ç¨‹â€æ•°é‡
DETAIL_WORKERS = 4       # é¢˜é¢è¯¦æƒ…çš„â€œæ ‡ç­¾é¡µçº¿ç¨‹â€æ•°é‡ï¼Œå¯ä¸ MAX_WORKERS ç›¸åŒ
OUTPUT_CSV = 'lanqiao_problems_all_with_detail.csv'   # è¾“å‡ºæ–‡ä»¶åï¼Œé¿å…è¦†ç›–ä½ ä¹‹å‰çš„ CSV


# ========================
# å·¥å…·å‡½æ•°
# ========================
def split_pages(max_page: int, n_workers: int):
    """æŠŠ 1..max_page å‡åŒ€åˆ†æˆ n_workers ä»½"""
    pages = list(range(1, max_page + 1))
    n_workers = min(n_workers, len(pages))
    size = math.ceil(len(pages) / n_workers)
    return [pages[i:i + size] for i in range(0, len(pages), size)]


def split_list(items, n_workers: int):
    """é€šç”¨åˆ—è¡¨åˆ‡åˆ†ï¼Œç”¨äºæŒ‰é¢˜ç›®åˆ‡åˆ†è¯¦æƒ…ä»»åŠ¡"""
    if not items:
        return []

    n_workers = min(n_workers, len(items))
    size = math.ceil(len(items) / n_workers)
    return [items[i:i + size] for i in range(0, len(items), size)]


def safe_pick_text(tab, selectors, timeout=0.5):
    """
    ä¾æ¬¡å°è¯•å¤šä¸ª CSS é€‰æ‹©å™¨ï¼Œè¿”å›ç¬¬ä¸€ä¸ªéç©ºçš„ .text
    è¿™é‡Œçš„ selectors æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯”å¦‚ ['div.problem-desc', 'div.problem-content']ã€‚
    """
    for sel in selectors:
        try:
            ele = tab.ele(f'css:{sel}', timeout=timeout)
            if ele:
                txt = (ele.text or '').strip()
                if txt:
                    return txt
        except Exception:
            # æŸäº›é€‰æ‹©å™¨æ‰¾ä¸åˆ°å…ƒç´ ä¼šæŠ›å¼‚å¸¸ï¼Œå¿½ç•¥ç»§ç»­è¯•ä¸‹ä¸€ä¸ª
            pass
    return ''


# ========================
# ç¬¬ä¸€é˜¶æ®µï¼šæŠ“å–é¢˜ç›®åˆ—è¡¨
# ========================
def crawl_pages(tab, pages_subset):
    """
    åœ¨ä¸€ä¸ªçº¿ç¨‹é‡Œä½¿ç”¨ä¸€ä¸ªæ ‡ç­¾é¡µ(tab)é‡‡é›†è‹¥å¹²é¡µã€‚
    tab: ChromiumTab æˆ– ChromiumPage å¯¹è±¡
    """
    thread_name = threading.current_thread().name
    print(f"[{thread_name}] åˆ—è¡¨çº¿ç¨‹å¯åŠ¨ï¼Œè´Ÿè´£é¡µç ï¼š{pages_subset[0]} ~ {pages_subset[-1]}")

    all_rows = []

    for p in pages_subset:
        url = BASE_URL.format(p)

        # æ¯ä¸€é¡µæœ€å¤šé‡è¯• 3 æ¬¡ï¼Œé˜²æ­¢ ContextLostError ç›´æ¥æŠŠç¨‹åºå¹²å´©
        for attempt in range(1, 4):
            try:
                print(f"[{thread_name}] æŠ“å–ç¬¬ {p} é¡µ (ç¬¬ {attempt} æ¬¡å°è¯•)ï¼š{url}")
                tab.get(url)
                tab.wait.doc_loaded()   # ç­‰é¡µé¢åŠ è½½å®Œæˆ
                time.sleep(0.8)         # ç¨å¾®æ­‡ä¸€ä¸‹ï¼Œé¿å…å¤ªçŒ›è§¦å‘é£æ§

                items = tab.eles('css:div.problem-item-wrapper')
                if not items:
                    print(f"[{thread_name}] âš ï¸ ç¬¬ {p} é¡µæ²¡æœ‰æŠ“åˆ°ä»»ä½•é¢˜ç›®ï¼ˆå¯èƒ½æ˜¯æœªç™»å½•/é£æ§/é¡µæ•°è¶…å‡ºï¼‰")
                    break

                print(f"[{thread_name}] âœ… ç¬¬ {p} é¡µå…± {len(items)} é“é¢˜")

                page_rows = []

                for item in items:
                    # é¢˜å·
                    id_ele = item.ele('css:span.problem-id', timeout=0)
                    pid = id_ele.text.strip() if id_ele else ''

                    # é¢˜ç›®åç§°
                    title_ele = item.ele('css:span.name.is-open', timeout=0)
                    if title_ele:
                        title = (title_ele.attr('title') or title_ele.text or '').strip()
                    else:
                        title = ''

                    # éš¾åº¦
                    level_ele = item.ele('css:span.level-text', timeout=0)
                    level = level_ele.text.strip() if level_ele else 'æœªçŸ¥'

                    # é€šè¿‡ç‡
                    percent_ele = item.ele('css:span.meta-percent', timeout=0)
                    percent = percent_ele.text.strip() if percent_ele else 'æœªçŸ¥'

                    # æŒ‘æˆ˜äººæ•°
                    user_ele = item.ele('css:span.meta-users', timeout=0)
                    users = user_ele.text.strip() if user_ele else 'æœªçŸ¥'

                    # æ ‡ç­¾
                    tag_eles = item.eles('css:div.problem-tags span.tag')
                    tags = [t.text.strip() for t in tag_eles]
                    tag_str = 'ã€'.join(tags) if tags else ''

                    page_rows.append((p, pid, title, level, percent, users, tag_str))

                all_rows.extend(page_rows)
                # å½“å‰é¡µæˆåŠŸäº†å°±è·³å‡ºé‡è¯•å¾ªç¯
                break

            except ContextLostError:
                # å®˜æ–¹è¯´æ˜ï¼šContextLostError æ˜¯â€œé¡µé¢è¢«åˆ·æ–°â€ï¼Œéœ€è¦ç­‰å¾…åé‡è¯•
                print(f"[{thread_name}] âš ï¸ ç¬¬ {p} é¡µå‡ºç° ContextLostErrorï¼Œå¯èƒ½æ˜¯é¡µé¢åˆ·æ–°äº†ï¼Œå‡†å¤‡é‡è¯•...")
                time.sleep(1.5)

                if attempt == 3:
                    print(f"[{thread_name}] âŒ ç¬¬ {p} é¡µé‡è¯• 3 æ¬¡ä»å¤±è´¥ï¼Œè·³è¿‡è¯¥é¡µã€‚")

            except Exception as e:
                print(f"[{thread_name}] âŒ ç¬¬ {p} é¡µå‡ºç°å…¶ä»–å¼‚å¸¸ï¼š{e}")
                if attempt == 3:
                    print(f"[{thread_name}] âŒ ç¬¬ {p} é¡µé‡è¯• 3 æ¬¡ä»å¤±è´¥ï¼Œè·³è¿‡è¯¥é¡µã€‚")
                else:
                    time.sleep(1.5)

    print(f"[{thread_name}] åˆ—è¡¨çº¿ç¨‹ç»“æŸï¼ŒæŠ“åˆ° {len(all_rows)} æ¡è®°å½•")
    return all_rows


def create_tabs(page: ChromiumPage, n_workers: int):
    """
    åœ¨ä¸€ä¸ªæµè§ˆå™¨é‡Œå¼€å¤šä¸ªæ ‡ç­¾é¡µï¼Œæ¯ä¸ªçº¿ç¨‹ç”¨ä¸€ä¸ªã€‚
    """
    tabs = []

    # ç¬¬ä¸€ä¸ªæ ‡ç­¾é¡µï¼šå½“å‰è¿™ä¸ª page æœ¬èº«å°±å¯ä»¥å½“ä½œä¸€ä¸ª tab ä½¿ç”¨
    tab0 = page.get_tab()
    tabs.append(tab0)

    # å†æ–°å»ºæ ‡ç­¾é¡µ
    for _ in range(n_workers - 1):
        # new_tab è¿”å›çš„æ˜¯ tab çš„ idï¼Œè¿™é‡Œå…ˆå»ºç©ºç™½é¡µï¼Œåç»­çº¿ç¨‹é‡Œå† get å®é™… URL
        tab_id = page.new_tab('about:blank')
        tab = page.get_tab(tab_id)
        tabs.append(tab)

    return tabs


async def async_crawl_all(page: ChromiumPage):
    """å¼‚æ­¥è°ƒåº¦ + çº¿ç¨‹æ± ï¼ŒæŠ“å®Œæ‰€æœ‰åˆ—è¡¨é¡µ"""
    page_splits = split_pages(MAX_PAGE, MAX_WORKERS)
    num_workers = len(page_splits)
    print(f"æ€»é¡µæ•° {MAX_PAGE}ï¼Œæ‹†æˆ {num_workers} ä¸ªåˆ—è¡¨ä»»åŠ¡ã€‚")

    # æŒ‰ä»»åŠ¡æ•°åˆ›å»ºå¯¹åº”æ•°é‡çš„æ ‡ç­¾é¡µ
    tabs = create_tabs(page, num_workers)

    loop = asyncio.get_running_loop()
    with ThreadPoolExecutor(max_workers=num_workers) as executor:
        tasks = [
            loop.run_in_executor(executor, crawl_pages, tabs[i], subset)
            for i, subset in enumerate(page_splits)
        ]
        all_batches = await asyncio.gather(*tasks)

    # ç»Ÿä¸€å…³æ‰å¤šå¼€çš„æ ‡ç­¾é¡µï¼ˆç¬¬ä¸€ä¸ª tab å°±æ˜¯ pageï¼Œå¯ä»¥ç•™ç€ä¹Ÿæ— æ‰€è°“ï¼‰
    for t in tabs[1:]:
        try:
            t.close()
        except Exception:
            pass

    return all_batches


def flatten_batches_to_problems(all_batches):
    """
    æŠŠäºŒç»´æ‰¹æ¬¡ all_batches å±•å¹³æˆæŒ‰é¢˜å·æ’åºçš„é¢˜ç›®åˆ—è¡¨ï¼Œ
    å¹¶è½¬æˆ dict ç»“æ„ï¼Œæ–¹ä¾¿åé¢å¡«å……é¢˜ç›®è¯¦æƒ…ã€‚
    """
    # å±•å¼€äºŒç»´åˆ—è¡¨
    rows = [row for batch in all_batches for row in batch]

    # æŒ‰ (é¡µç , é¢˜å·) æ’åºï¼Œä¿è¯é¡ºåºç¨³å®š
    rows.sort(key=lambda x: (x[0], int(x[1]) if str(x[1]).isdigit() else 999999))

    problems = []
    for idx, (page_no, pid, title, level, percent, users, tag_str) in enumerate(rows, start=1):
        problems.append(
            {
                'idx': idx,
                'page_no': page_no,
                'pid': pid,
                'title': title,
                'level': level,
                'percent': percent,
                'users': users,
                'tags': tag_str,
                # ä»¥ä¸‹å­—æ®µç¬¬äºŒé˜¶æ®µå†å¡«
                'detail_url': '',
                'desc': '',
                'input_desc': '',
                'output_desc': '',
                'sample_input': '',
                'sample_output': '',
            }
        )

    return problems


# ========================
# ç¬¬äºŒé˜¶æ®µï¼šæŠ“å–é¢˜ç›®è¯¦æƒ…ï¼ˆé¢˜é¢ï¼‰
# ========================
def parse_problem_detail(tab):
    """
    ä»ã€é¢˜ç›®è¯¦æƒ…é¡µã€‘æå–é¢˜ç›®æè¿°ã€è¾“å…¥è¾“å‡ºè¯´æ˜ã€æ ·ä¾‹ç­‰ã€‚

    âš ï¸ æ³¨æ„ï¼šä¸‹é¢è¿™äº› CSS é€‰æ‹©å™¨æ˜¯æ ¹æ®å¸¸è§ç»“æ„çŒœçš„ï¼Œ
    å¦‚æœæŠ“åˆ°çš„å†…å®¹ä¸ºç©ºï¼Œä½ å¯ä»¥ï¼š
      1. æ‰‹åŠ¨æ‰“å¼€ä¸€ä¸ªé¢˜ç›®é¡µé¢ï¼ˆæ¯”å¦‚ 20512ï¼‰;
      2. F12 æŸ¥çœ‹â€œé¢˜ç›®æè¿°â€â€œè¾“å…¥è¯´æ˜â€ç­‰æ‰€åœ¨çš„ div çš„ classï¼›
      3. æŠŠå¯¹åº” class åè¡¥è¿›ä¸‹é¢çš„ selectors åˆ—è¡¨é‡Œã€‚
    """

    # é¢˜ç›®æè¿°
    desc = safe_pick_text(
        tab,
        [
            'div.problem-desc',
            'div.problem-content',
            'div.desc',
            'div.markdown-body',
            'section.problem-content',
        ]
    )

    # è¾“å…¥è¯´æ˜
    input_desc = safe_pick_text(
        tab,
        [
            'div.problem-input',
            'section.problem-input',
            'div.input-spec',
            'div.input > pre',
        ]
    )

    # è¾“å‡ºè¯´æ˜
    output_desc = safe_pick_text(
        tab,
        [
            'div.problem-output',
            'section.problem-output',
            'div.output-spec',
            'div.output > pre',
        ]
    )

    # æ ·ä¾‹è¾“å…¥
    sample_input = safe_pick_text(
        tab,
        [
            'pre.sample-input',
            'div.sample-input pre',
            'code.sample-input',
        ]
    )

    # æ ·ä¾‹è¾“å‡º
    sample_output = safe_pick_text(
        tab,
        [
            'pre.sample-output',
            'div.sample-output pre',
            'code.sample-output',
        ]
    )

    return desc, input_desc, output_desc, sample_input, sample_output


def crawl_detail_batch(tab, problems_subset):
    """
    åœ¨ä¸€ä¸ªçº¿ç¨‹é‡Œä½¿ç”¨ä¸€ä¸ªæ ‡ç­¾é¡µ(tab)é‡‡é›†è‹¥å¹²ã€é¢˜ç›®è¯¦æƒ…é¡µã€‘ã€‚
    """
    thread_name = threading.current_thread().name
    print(f"[{thread_name}] è¯¦æƒ…çº¿ç¨‹å¯åŠ¨ï¼Œè¦æŠ“ {len(problems_subset)} é“é¢˜ç›®ã€‚")

    for problem in problems_subset:
        pid = problem['pid']
        if not pid:
            continue

        url = DETAIL_URL.format(pid=pid)
        problem['detail_url'] = url

        for attempt in range(1, 4):
            try:
                print(f"[{thread_name}] æŠ“å–é¢˜å· {pid} çš„é¢˜é¢ (ç¬¬ {attempt} æ¬¡å°è¯•)ï¼š{url}")
                tab.get(url)
                tab.wait.doc_loaded()
                time.sleep(0.8)  # è§†æƒ…å†µå¯è°ƒå°æˆ–è°ƒå¤§

                desc, input_desc, output_desc, sample_in, sample_out = parse_problem_detail(tab)

                problem['desc'] = desc
                problem['input_desc'] = input_desc
                problem['output_desc'] = output_desc
                problem['sample_input'] = sample_in
                problem['sample_output'] = sample_out

                # æ­£å¸¸æŠ“åˆ°å°± break
                break

            except ContextLostError:
                print(f"[{thread_name}] âš ï¸ é¢˜å· {pid} å‡ºç° ContextLostErrorï¼Œå‡†å¤‡é‡è¯•...")
                time.sleep(1.5)
                if attempt == 3:
                    print(f"[{thread_name}] âŒ é¢˜å· {pid} é‡è¯• 3 æ¬¡ä»å¤±è´¥ï¼Œè·³è¿‡è¯¥é¢˜ç›®ã€‚")

            except Exception as e:
                print(f"[{thread_name}] âŒ æŠ“å–é¢˜å· {pid} æ—¶å‡ºç°å¼‚å¸¸ï¼š{e}")
                if attempt == 3:
                    print(f"[{thread_name}] âŒ é¢˜å· {pid} é‡è¯• 3 æ¬¡ä»å¤±è´¥ï¼Œè·³è¿‡è¯¥é¢˜ç›®ã€‚")
                else:
                    time.sleep(1.5)

    print(f"[{thread_name}] è¯¦æƒ…çº¿ç¨‹ç»“æŸã€‚")
    return problems_subset


def crawl_all_details(page: ChromiumPage, problems, max_workers: int = DETAIL_WORKERS):
    """
    å¤šçº¿ç¨‹ + å¤šæ ‡ç­¾é¡µï¼ŒæŠŠæ¯é“é¢˜çš„ã€é¢˜é¢è¯¦æƒ…ã€‘æŠ“å‡ºæ¥ï¼Œå¡«å› problems åˆ—è¡¨ã€‚
    """
    if not problems:
        return problems

    splits = split_list(problems, max_workers)
    num_workers = len(splits)
    print(f"å…± {len(problems)} é“é¢˜ï¼Œæ‹†æˆ {num_workers} ä¸ªè¯¦æƒ…ä»»åŠ¡ã€‚")

    tabs = create_tabs(page, num_workers)

    results = []
    with ThreadPoolExecutor(max_workers=num_workers) as executor:
        futures = [
            executor.submit(crawl_detail_batch, tabs[i], subset)
            for i, subset in enumerate(splits)
        ]
        for fut in futures:
            results.append(fut.result())

    # ç»Ÿä¸€å…³æ‰å¤šå¼€çš„æ ‡ç­¾é¡µï¼ˆç¬¬ä¸€ä¸ª tab å°±æ˜¯ pageï¼Œå¯ä»¥ç•™ç€ä¹Ÿæ— æ‰€è°“ï¼‰
    for t in tabs[1:]:
        try:
            t.close()
        except Exception:
            pass

    # åˆå¹¶ç»“æœï¼Œå¹¶æŒ‰åŸ idx æ’åº
    merged = [item for batch in results for item in batch]
    merged.sort(key=lambda x: x['idx'])
    return merged


# ========================
# ä¿å­˜åˆ° CSV
# ========================
def save_to_csv(problems):
    print("-" * 80)
    print(f"æ±‡æ€»åå…± {len(problems)} é“é¢˜ç›®ï¼ˆå«é¢˜é¢ï¼‰ï¼Œå¼€å§‹å†™å…¥ CSVï¼š{OUTPUT_CSV}")

    with open(OUTPUT_CSV, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.writer(f)
        writer.writerow(
            [
                'åºå·',
                'é¢˜å·',
                'é¢˜ç›®åç§°',
                'éš¾åº¦',
                'é€šè¿‡ç‡',
                'æŒ‘æˆ˜äººæ•°',
                'æ ‡ç­¾',
                'æ‰€åœ¨é¡µç ',
                'é¢˜ç›®é“¾æ¥',
                'é¢˜ç›®æè¿°',
                'è¾“å…¥è¯´æ˜',
                'è¾“å‡ºè¯´æ˜',
                'æ ·ä¾‹è¾“å…¥',
                'æ ·ä¾‹è¾“å‡º',
            ]
        )

        for item in problems:
            writer.writerow(
                [
                    item['idx'],
                    item['pid'],
                    item['title'],
                    item['level'],
                    item['percent'],
                    item['users'],
                    item['tags'],
                    item['page_no'],
                    item.get('detail_url') or DETAIL_URL.format(pid=item['pid']),
                    item.get('desc', ''),
                    item.get('input_desc', ''),
                    item.get('output_desc', ''),
                    item.get('sample_input', ''),
                    item.get('sample_output', ''),
                ]
            )

    print("ğŸ’¾ å†™å…¥å®Œæˆï¼")


# ========================
# ä¸»å…¥å£
# ========================
if __name__ == '__main__':
    t0 = time.time()
    print("å¼€å§‹å¤šçº¿ç¨‹ + asyncio æŠ“å–è“æ¡¥é¢˜ç›®ã€åˆ—è¡¨ + é¢˜é¢ã€‘...")

    # åªåˆ›å»ºä¸€ä¸ªé¡µé¢å¯¹è±¡ï¼Œåé¢æ‰€æœ‰çº¿ç¨‹éƒ½é€šè¿‡ä¸åŒ tab æ¥ç”¨å®ƒ
    page = ChromiumPage()

    try:
        # ç¬¬ä¸€é˜¶æ®µï¼šåˆ—è¡¨é¡µ
        list_batches = asyncio.run(async_crawl_all(page))
        problems = flatten_batches_to_problems(list_batches)

        # ç¬¬äºŒé˜¶æ®µï¼šé¢˜ç›®è¯¦æƒ…é¡µï¼ˆé¢˜é¢ï¼‰
        problems = crawl_all_details(page, problems)

        # ä¿å­˜åˆ° CSV
        save_to_csv(problems)

    finally:
        # åªåœ¨ä¸»çº¿ç¨‹ç»Ÿä¸€ quit ä¸€æ¬¡
        try:
            page.quit()
        except Exception:
            pass

    t1 = time.time()
    print(f"å…¨éƒ¨å®Œæˆï¼Œç”¨æ—¶ {t1 - t0:.2f} ç§’")
